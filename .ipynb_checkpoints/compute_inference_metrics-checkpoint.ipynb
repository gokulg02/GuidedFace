{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60a7c35f-f996-4349-bd17-930f34d35c21",
   "metadata": {},
   "source": [
    "## Compute FID and IS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7208f6e8-2013-4a93-a053-d89d5a351a1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating feature extractor \"inception-v3-compat\" with features ['logits_unbiased', '2048']\n",
      "Downloading: \"https://github.com/toshas/torch-fidelity/releases/download/v0.2.0/weights-inception-2015-12-05-6726825d.pth\" to /tmp/xdg-cache/torch/hub/checkpoints/weights-inception-2015-12-05-6726825d.pth\n",
      "100%|██████████| 91.2M/91.2M [00:02<00:00, 32.1MB/s]\n",
      "Extracting features from input1\n",
      "Looking for samples non-recursivelty in \"./training/celeb/target\" with extensions png,jpg,jpeg\n",
      "Found 29344 samples, some are lossy-compressed - this may affect metrics\n",
      "/home/ggandhikumar/.local/lib/python3.11/site-packages/torch_fidelity/datasets.py:16: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  img = torch.ByteTensor(torch.ByteStorage.from_buffer(img.tobytes())).view(height, width, 3)\n",
      "Processing samples                                                             \n",
      "Extracting features from input2\n",
      "Looking for samples non-recursivelty in \"./training/celeb/generated\" with extensions png,jpg,jpeg\n",
      "Found 1571 samples, some are lossy-compressed - this may affect metrics\n",
      "Processing samples                                                           \n",
      "Inception Score: 3.671320496860079 ± 0.04729767050000686\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FID: 31.0358\n",
      "Inception Score (Mean): 3.6713\n",
      "Inception Score (Std): 0.0473\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Frechet Inception Distance: 31.035827041035446\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch_fidelity import calculate_metrics\n",
    "\n",
    "metrics = calculate_metrics(\n",
    "    input1='./training/target_subset', #real images\n",
    "    input2='./training/generated', #generated images\n",
    "    cuda=torch.cuda.is_available(),\n",
    "    isc=True,\n",
    "    fid=True\n",
    ")\n",
    "\n",
    "print(f\"FID: {metrics['frechet_inception_distance']:.4f}\")\n",
    "print(f\"Inception Score (Mean): {metrics['inception_score_mean']:.4f}\")\n",
    "print(f\"Inception Score (Std): {metrics['inception_score_std']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe0a1ebd-7f3c-4df8-b93b-5966895d2675",
   "metadata": {},
   "source": [
    "## Compute Similarity Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a305946-6b30-411f-a374-48cf0a4a334f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average similarity across 1973 image pairs: 0.9961\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import dlib\n",
    "import numpy as np\n",
    "from scipy.spatial import procrustes\n",
    "\n",
    "# === Paths ===\n",
    "input_dir_1 = './training/generated' #generated\n",
    "input_dir_2 = './training/target' # real\n",
    "\n",
    "# === Dlib Face Detector and Landmark Predictor ===\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(\"models/shape_predictor_68_face_landmarks.dat\")  # Make sure this is in your directory\n",
    "\n",
    "def get_landmarks(image_path):\n",
    "    img = cv2.imread(image_path)\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    faces = detector(gray)\n",
    "    \n",
    "    if len(faces) == 0:\n",
    "        return None\n",
    "\n",
    "    shape = predictor(gray, faces[0])\n",
    "    landmarks = [(p.x, p.y) for p in shape.parts()]\n",
    "    return np.array(landmarks)\n",
    "\n",
    "def procrustes_similarity(lm1, lm2):\n",
    "    try:\n",
    "        _, _, disparity = procrustes(lm1, lm2)\n",
    "        return 1 - disparity  # higher = more similar\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "# === Main Loop ===\n",
    "similarity_scores = []\n",
    "\n",
    "for filename in os.listdir(input_dir_1):\n",
    "    path1 = os.path.join(input_dir_1, filename)\n",
    "    path2 = os.path.join(input_dir_2, filename)\n",
    "\n",
    "    if not os.path.exists(path2):\n",
    "        continue\n",
    "\n",
    "    landmarks1 = get_landmarks(path1)\n",
    "    landmarks2 = get_landmarks(path2)\n",
    "\n",
    "    if landmarks1 is None or landmarks2 is None:\n",
    "        continue\n",
    "\n",
    "    similarity = procrustes_similarity(landmarks1, landmarks2)\n",
    "    if similarity is not None:\n",
    "        similarity_scores.append(similarity)\n",
    "\n",
    "# === Compute Average ===\n",
    "if similarity_scores:\n",
    "    avg_similarity = np.mean(similarity_scores)\n",
    "    print(f\"Average similarity across {len(similarity_scores)} image pairs: {avg_similarity:.4f}\")\n",
    "else:\n",
    "    print(\"No valid similarities calculated.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
